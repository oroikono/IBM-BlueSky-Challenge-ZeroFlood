{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4AJmE3mC0Yi"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "49zZx_JaC0Yj"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lflY-hyLKp0b",
        "outputId": "cd85a095-0c56-40d3-cc64-6585d75652cf"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhQp-UsIDxv5"
      },
      "source": [
        "## Install packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dWgIALAEC0Yk"
      },
      "outputs": [],
      "source": [
        "! pip install terratorch gdown tensorboard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__neCABcC0Yk"
      },
      "source": [
        "## Import modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GuFr6TqrC0Yk"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import jaccard_score\n",
        "from sklearn.metrics import f1_score\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from dataset import FloodDataModule\n",
        "from model import FloodRiskModel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3esLWgID-_v"
      },
      "source": [
        "## Download dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWTGu5ddQWQy",
        "outputId": "fc854a53-d58e-4a94-860a-2722404ddb90"
      },
      "outputs": [],
      "source": [
        "! git lfs install\n",
        "! git clone https://huggingface.co/datasets/hk-kaden-kim/BlueSky-Challenge-Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fFbjPdQlELXD",
        "outputId": "1a40ad31-eb27-4d7a-c5d2-1b7f3aa3d015"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train: 3184 images and 3184 masks\n",
            "test: 1332 images and 1332 masks\n",
            "val: 2128 images and 2128 masks\n"
          ]
        }
      ],
      "source": [
        "# Dataset statistics\n",
        "DATASET_DIR = \"BlueSky-Challenge-Dataset\"\n",
        "for split in [\"train\", \"test\", \"val\"]:\n",
        "  png_files_cnt = 0\n",
        "  csv_files_cnt = 0\n",
        "  for root, dirs, files in os.walk(os.path.join(DATASET_DIR, split)):\n",
        "      for file in files:\n",
        "          if file.endswith('.png'):\n",
        "              png_files_cnt += 1\n",
        "          if file.endswith('.csv'):\n",
        "              csv_files_cnt += 1\n",
        "  print(f\"{split}: {png_files_cnt} images and {csv_files_cnt} masks\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yWYhC4qC0Yl"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMspDscNC0Ym"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 4\n",
        "CHECKPOINT = \"...\"\n",
        "TiM = [\n",
        "    \"S1RTC\",\n",
        "    \"DEM\",\n",
        "    \"LULC\",\n",
        "    \"NDVI\",\n",
        "    ]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SxGjyXMC0Ym",
        "outputId": "5fa667b4-eca8-4c4f-b82f-c155bd3622b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation samples: 1332\n",
            "\n",
            "Dataset Summary:\n",
            "Test samples: 1332\n",
            "Sample keys: dict_keys(['image', 'mask'])\n",
            "Image shape: torch.Size([3, 256, 256])\n",
            "Mask shape: torch.Size([256, 256])\n",
            "Mask unique values: tensor([0, 1])\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Initialize datamodule\n",
        "datamodule = FloodDataModule(\n",
        "    local_path=DATASET_DIR,\n",
        "    batch_size=BATCH_SIZE,  # Adjust based on GPU memory\n",
        "    num_workers=8\n",
        ")\n",
        "datamodule.setup(\"test\")\n",
        "sample = datamodule.test_dataset[0]\n",
        "\n",
        "print(f\"\\nDataset Summary:\")\n",
        "print(f\"Test samples: {len(datamodule.test_dataset)}\")\n",
        "print(f\"Sample keys: {sample.keys()}\")\n",
        "print(f\"Image shape: {sample['image'].shape}\")\n",
        "print(f\"Mask shape: {sample['mask'].shape}\")\n",
        "print(f\"Mask unique values: {torch.unique(sample['mask'])}\")\n",
        "print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "enEpoT9-Kt0D",
        "outputId": "9d763e71-bb61-4f56-b15a-6a82eeb2fd6c"
      },
      "outputs": [],
      "source": [
        "# Initialize model\n",
        "model = FloodRiskModel.load_from_checkpoint(CHECKPOINT, tim=TiM)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "_ = model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kcr8xl-aC0Yn",
        "outputId": "4592e270-9fbf-43fd-cd30-496d255f761c"
      },
      "outputs": [],
      "source": [
        "# Inference\n",
        "all_results = []\n",
        "with torch.no_grad():\n",
        "    for batch_idx, batch in enumerate(tqdm(datamodule.test_dataloader())):\n",
        "\n",
        "        imgs = batch['image'].to(device)\n",
        "        logits = model(imgs).output\n",
        "\n",
        "        probs = torch.softmax(logits, dim=1)\n",
        "        confs = probs[:,1].cpu().numpy()\n",
        "\n",
        "        pred_masks = torch.argmax(probs, dim=1).cpu()\n",
        "        true_masks = batch['mask']\n",
        "\n",
        "        # compute the starting index for this batch\n",
        "        base_idx = batch_idx * datamodule.batch_size\n",
        "\n",
        "        for i in range(imgs.size(0)):\n",
        "\n",
        "            sample_idx = base_idx + i\n",
        "            img_path   = datamodule.test_dataset.samples[sample_idx][0]\n",
        "\n",
        "            pred_flatten  = pred_masks[i].numpy().flatten()\n",
        "            true_flatten = true_masks[i].numpy().flatten()\n",
        "\n",
        "            all_results.append({\n",
        "                'image_path':     img_path,\n",
        "                'accuracy':       accuracy_score(true_flatten, pred_flatten),\n",
        "                'iou':            jaccard_score(true_flatten, pred_flatten, average='binary'),\n",
        "                'f1':             f1_score(true_flatten, pred_flatten, average='binary'),\n",
        "                'flood_pct_pred': pred_flatten.sum() / pred_flatten.size * 100,\n",
        "                'flood_pct_gt':   true_flatten.sum() / true_flatten.size * 100,\n",
        "                'flood_mask':     pred_masks[i].numpy(),\n",
        "                'confidence':     confs[i]\n",
        "            })\n",
        "\n",
        "print(f\"Tested {len(all_results)} samples.\")\n",
        "print(f\"Mean Acc: {np.mean([r['accuracy'] for r in all_results]):.3f}\")\n",
        "print(f\"Mean IoU: {np.mean([r['iou']     for r in all_results]):.3f}\")\n",
        "print(f\"Mean F1:  {np.mean([r['f1']      for r in all_results]):.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y6ijgaPBC0Yn"
      },
      "outputs": [],
      "source": [
        "TEST_SAMPLES = ['acbe', 'ahaw', 'ayvf', 'bctf', 'biqb', 'blad', 'bmwg', 'cafp', 'cbre', 'ccea',\n",
        "                'cdhx', 'cnyv', 'crgs', 'cxsh', 'cyrz', 'czjn', 'ddwt', 'dhkk', 'dwge', 'dwgx',\n",
        "                'dxqd', 'emqi', 'fajg', 'faqi', 'fdjr', 'fixa', 'fnho', 'gfky', 'gjeh', 'glsx',\n",
        "                'gmwz', 'gnrt', 'gtrx', 'gvqo', 'gykv', 'hehl', 'hghn', 'hueg', 'igwf', 'ikfi',\n",
        "                'iusn', 'iyqz', 'kmfc', 'lgyh', 'ljwq', 'lxlx', 'mjqy', 'mjwv', 'mkdo', 'mmnr',\n",
        "                'mmwp', 'neqf', 'nezm', 'nrai', 'ntjc', 'ntxe', 'nwhe', 'ogpq', 'omqk', 'ourg',\n",
        "                'paty', 'pdfj', 'pevf', 'pfmg', 'qasf', 'qdta', 'qszb', 'qxrg', 'rhcz', 'riwa',\n",
        "                'roic', 'rxrr', 'skos', 'soul', 'uhce', 'wrex', 'xbqv', 'xfon', 'xsfm', 'ymhx',\n",
        "                'ynon', 'yxeg', 'zean', 'zjkl', 'zkmf', 'zoan',]\n",
        "\n",
        "test_samples = TEST_SAMPLES[:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tEPN6BVdX4kI",
        "outputId": "7cb503fc-bfc8-4ebb-9763-b083abfb0e0b"
      },
      "outputs": [],
      "source": [
        "# Create subplots\n",
        "rows = len(test_samples)\n",
        "cols = 3\n",
        "fig, axes = plt.subplots(rows, cols, figsize=(cols * 3, rows * 3))\n",
        "if rows == 1:\n",
        "    axes = [axes]\n",
        "\n",
        "zone_id_all = [r['image_path'].name for r in all_results]\n",
        "for i, zone_id in enumerate(test_samples):\n",
        "\n",
        "  idx = zone_id_all.index(zone_id+'.png')\n",
        "\n",
        "  # Prediction - Flood Risky Zone\n",
        "  pred_arr = all_results[idx]['flood_mask']\n",
        "  pred_rgba = np.zeros((*pred_arr.shape, 4), dtype=np.float32)\n",
        "  pred_rgba[pred_arr == 1] = [1, 0, 0, 0.3]     # Red\n",
        "\n",
        "  # Satellite RGB Imagery\n",
        "  img_path = str(all_results[idx]['image_path'])\n",
        "  image = Image.open(img_path).convert('RGB')\n",
        "  image_resized = image.resize((pred_arr.shape[1], pred_arr.shape[0]), Image.NEAREST)\n",
        "  image_resized = np.array(image_resized)\n",
        "\n",
        "  # Ground Truth - Flood / Water Body / Water & Flood\n",
        "  mask_path = img_path.replace('image','mask').replace('png','csv')\n",
        "  mask_arr = pd.read_csv(mask_path,header=None).to_numpy()\n",
        "  K = int(pred_arr.shape[0]/ mask_arr.shape[0])\n",
        "  mask_arr = mask_arr.repeat(K, axis=0).repeat(K, axis=1)\n",
        "  mask_rgba = np.zeros((*mask_arr.shape, 4), dtype=np.float32)\n",
        "  mask_rgba[mask_arr == 1] = [1, 0, 0, 0.3]     # Red\n",
        "  mask_rgba[mask_arr == 2] = [0, 0, 1, 0.3]     # Blue\n",
        "  mask_rgba[mask_arr == 3] = [1, 1, 0, 0.3]     # Yellow\n",
        "\n",
        "  # Plot\n",
        "  axes[i][0].imshow(image_resized)\n",
        "  axes[i][0].set_title(f\"{zone_id}\")\n",
        "  axes[i][0].axis('off')\n",
        "\n",
        "  axes[i][1].imshow(image_resized)\n",
        "  axes[i][1].imshow(mask_rgba)\n",
        "  axes[i][1].set_title(f'Ground Truth')\n",
        "  axes[i][1].axis('off')\n",
        "\n",
        "  axes[i][2].imshow(image_resized)\n",
        "  axes[i][2].imshow(pred_rgba)\n",
        "  axes[i][2].set_title(f'Predicted')\n",
        "  axes[i][2].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
